---
title: "Decolonial AI: Decolonial Theory as Sociotechnical Foresight in Artificial Intelligence"
date: 2020-11-22T12:45:03-06:00
draft: False
tags: ['ai policy', 'Race']
categories: ['Papers']
---

By [Shakir Mohamed](https://www.shakirm.com/), [Marie-Therese Png](https://www.oii.ox.ac.uk/people/marie-therese-png/), and [William Isaac](https://wsisaac.com/#)

[Paper Link](https://arxiv.org/abs/2007.04068)

## Paper Motivation

Artificial Intelligence has the potential to re-shape our society. These changes could be broadly beneficial but aslo have the potential to be dangerous for vulnerable peoples. To understand the impacts of AI we need to think about values and power. It is the responsibility of the AI community to have ethical foresight and involve many different perspectives.

## Paper Contribution

The paper explores using post-colonial and decolonial theories to understand advances in AI. AI communities can use decolonial theory to have better insight about how to align technology with ethical principles that center vulnerable peoples. The authors highlight problematic technology that are instances of coloniality.

The authors suggest the following three practices  to incorporate decolonial theory into the field of artificial intelligence

1. Creating a critical technical practice of AI
2. Seeking reverse tutelage and reverse pedagogies
3. The renewal of affective and political communities

## Background

AI can be seen as both an object and a subject. It is both a technological artifact and a system of networks and institutions. Recent advances in AI have shown both the potential benefits and the potential harms. What values and norms should we aim to uphold when using AI?

Science is a product of both epistemic values (consistency, generalizability, falsifiability) and contextual values that are moral, societal and personal. These contextual values play a large role in choice of research projects and application of results.

Due to previous research malpractice the scientific community has developed three core ethical values for human-subject studies.

1. respect for persons
2. beneficience
3. justice

While these ethical values are a good starting point, the limitations of them has become clear as AI has advanced. We have seen AI algorithms repeatedly exhibit racial bias when deployed in real systems. These problems exacerbate existing structural inequalities. AI also makes it easier to unknowingly adopt and perpetuate existing social biases. AI therefore requires an expansion of ethical frameworks.

A critical science approach seeks to uncover the underlying cultural assumptions that dominate a field. Technological foresight and foresight methodologies are used to understand and anticipate how choices today will impact the future.

## Coloniality and Decolonial Theory

decolonization is concerned with the restoration of land and life following the end of colonialism. Colonialism's impacts still endure until this day. Coloniality highlights the continued patterns of power between coloniser and colonised. Decolonization is an attempt to remove the lasting impact of colonial relations. It hopes to undo mechanisms of power, economics, language and culture than continue to shape contemporary life.

- a __decentering view__ rejects an imitation of the West in all aspects of life. We must restore global histories.
- a __addidtive-inclusive view__ continues to use existing knowledge but specifically recognizes the value of alternative approaches.
- an __engagement view__ asks us to be more directly critical of science. It asks us to examine science from the margins.

There are many frameworks that come out of decolonial theory which ask us to challenge the current centers of power. Centers of power can be called Metropoles. Simply looking at metropole-periphery dichotomes can over simplify lived experience. Due to some limitations of decolonial theories the authors want to incorporate a wider critical science view


## Algorithmic Coloniality

Digital spaces, similar to physical spaces, often become sites of extraction and exploitation. Data must be recognized as a material resource than can be exploited for economic gain. Algorithms now impact the allocation of resources and discriminatory systems. The discussions of power and inequality with regard to AI cannot be seen as ahistorical. We must recognize colonial continuities.

**Algorithmic Decision Systems**

Systems leveraging AI have led to new types of policing and surveillance. The initial goal of these tools was to provide decision-support that is evidence driven and unbiased. Yet there has been lots of evidence that depicts a reality which is the opposite. Applications of facial recognition technology have been shown to entrench historical injustices. This has often been because of social biases in data. Harms from AI need to be understand as continuous to histories of racist expropriation.

**Algorithmic Exploitation**

Many recent successes in AI can be attributed to large volumes of labeled data. There are people who do this labeling for a living. In the most extreme cases this labeling has been done by prisoners and those who are economically vulnerable. This establishes a form of knowledge and labour extraction that is paid at very low rates. A decolonial views forces us to consider how colonial history impacts present day labour regulation and enforcement.

There is a long history of exploiting marginalized groups for the purpose of scientific progress. There is a history of experimentation on African Americans in the United States. We must use this lens to understand beta-testing, the fine-tuning of early versions of software systems. Organizations often use countries outside of their own as testing grounds. The firm Palantir experimentally deployed predictive policing algorithms in the city of New Orleans without public approval.

**Algorithmic Dispossession**

Certain regulatory policies result in a centralization of power in the hands of a minority. Global AI ethics guidelines have been developed with under representation of Africa, South and Central America and Central Asia. More economically developed countries are shaping guidelines without considering other countries. There are clear hierarchies of power that play a role in policy development.

We need to expand the definition of what ‘social good‘ means in the field of computer science. We want to create systems that promote an active and engaged political community. AI needs to be designed and deployed by the communities it will work for. This requires adopting co-development strategies.

## Tactics for Decolonial AI

We aim to decolonize power to dismantle harmful power asymmetries. We should acknowledge a larger radius of socio-politcal, ecological cultural and economic needs. These tactics are not a conclusive solution but open up different approaches and narratives.

**1. A critical Technical practice of AI**

We should recognize power imbalances and their implicit value systems. This work takes a middle ground between developing new algorithms and finding hidden assumptions. There is a limitation to purely technical approaches. We must consider larger contexts that look at the interplay of social, cultural and technical elements.

- Fairness: Developing a fair classifier can still lead to a discriminatory practice due to underlying power dynamics. The definition of fairness is a function of social and political factors.
- Safety: How do we build AI systems‘ whose values align with our own. There can be a mismatch between the revealed specification of AI and the ideal specification.  We must examine whose values and goals are represented in our training process.
- Diversity: Diversity to build more effective teams is not a critical practice. Only through diversity can we confront issues of homogenization, power, values and cultural colonialism. Diversity allows for intersectional approaches to problem solving.
- Policy: We should support localized AI development.
- Resistance: Can AI itself be used as a decolonising tool.


**2. Reciprocal Engagements and Reverse Tutelage**

There is important history that shows how the metropole learned from peripheries. Reverse tutelage asks us to question what constitutes knowledge. Deciding what data goes into a data set and what does not is a way of specifying which types of knowledge we value.
- Dialogue: There is not one universal way of looking at things. We must create a dialogue between different forms of knowledge.
- Documentation: Documentation allows assumptions about systems to be made public and interrogated.
- Design: There are frameworks for community engaged research.

**3. Renewed Affective and Political Communities**

We must put structures in places for communities to contest, redress and reverse technological interventions.


## My Closing Thoughts

One theme that jumped out to me from the paper is the need to be proactive rather than re-active. We can use the tools from this paper to do a better job of having foresight. We should not be surprised when tools are misused or when they perpetuate existing power differences.

My friend [Tushar](https://tusharc.dev/) had a helpful framing of this paper. We are often thinking simply in our current ethical context. What seems ethical based on moral values today. But this paper challenges us to think about the historical context as well. It is not enough to only think about our current situation. We have to understand our current situation as coming from that historical one. We need to be aware of the historical context and apply it.

I really appreciate bringing another field and applying it to AI and using that to challenge the field of AI. As someone who is not as familiar with critical science or critical theory, I am still struggling to define exactly what it is. My current understanding from the paper and some cursory research is that critical science involves inspecting power structures and societal structures rather than just individuals. Critical science allows us to better understand the adverse consequences of technological development.

